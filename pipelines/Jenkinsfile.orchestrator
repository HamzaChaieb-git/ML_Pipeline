pipeline {
    agent any
    
    environment {
        BASE_IMAGE = 'hamzachaieb01/ml-pipeline'
        DATA_IMAGE = 'hamzachaieb01/ml-data'
        MODEL_IMAGE = 'hamzachaieb01/ml-model'
        TAG = 'latest'
        WORKSPACE_SAFE = sh(script: "echo '${WORKSPACE}' | sed 's/ /\\\\ /g'", returnStdout: true).trim()
        MLFLOW_TRACKING_URI = 'http://mlflow_server:5001'
        EXPERIMENT_NAME = 'churn_prediction'
        MLFLOW_DB_DIR = "${WORKSPACE_SAFE}/mlflow_data"
    }
    
    options {
        timeout(time: 2, unit: 'HOURS')
        disableConcurrentBuilds()
    }
    
    stages {
        stage('Setup MLflow') {
            steps {
                script {
                    // Cleanup without sudo
                    sh '''
                        docker rm -f mlflow_server model_trainer data_container || true
                        docker network rm mlflow_net || true
                        rm -rf ${WORKSPACE}/mlflow_data || true
                    '''
                    
                    // Setup directories
                    sh """
                        docker network create mlflow_net || true
                        
                        # Create fresh mlflow directories
                        mkdir -p "${MLFLOW_DB_DIR}"/{database,artifacts}
                        chmod -R 777 "${MLFLOW_DB_DIR}"
                    """
                    
                    // Initialize MLflow environment
                    sh """
                        # Start MLflow server with database initialization
                        docker run -d \
                            --name mlflow_server \
                            --network mlflow_net \
                            -p 5001:5001 \
                            -v "${MLFLOW_DB_DIR}/database":/mlflow_data/database \
                            -v "${MLFLOW_DB_DIR}/artifacts":/mlflow_data/artifacts \
                            ${BASE_IMAGE}:${TAG} \
                            bash -c "
                                mkdir -p /mlflow_data/database /mlflow_data/artifacts && \
                                chmod -R 777 /mlflow_data && \
                                mlflow server \
                                    --backend-store-uri sqlite:///mlflow_data/database/mlflow.db \
                                    --default-artifact-root /mlflow_data/artifacts \
                                    --host 0.0.0.0 \
                                    --port 5001 \
                                    --workers 1
                            "
                    """
                    
                    // Verify MLflow
                    sh '''
                        echo "Waiting for MLflow server..."
                        sleep 15
                        
                        MAX_RETRIES=5
                        RETRY_COUNT=0
                        
                        while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                            if curl -s http://localhost:5001/api/2.0/mlflow/experiments/list > /dev/null; then
                                echo "MLflow server is ready"
                                break
                            fi
                            
                            RETRY_COUNT=$((RETRY_COUNT + 1))
                            if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
                                echo "MLflow server failed to start. Logs:"
                                docker logs mlflow_server
                                exit 1
                            fi
                            
                            echo "Retry $RETRY_COUNT of $MAX_RETRIES. Waiting 5 seconds..."
                            sleep 5
                        done
                    '''
                }
            }
        }

        stage('Train Model') {
            steps {
                script {
                    try {
                        // Setup containers
                        sh """
                            # Setup data container
                            docker create --name data_container \
                                --network mlflow_net \
                                ${DATA_IMAGE}:${TAG}
                            
                            # Setup model trainer
                            docker run -d \
                                --name model_trainer \
                                --network mlflow_net \
                                -e MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
                                -e MLFLOW_EXPERIMENT_NAME=${EXPERIMENT_NAME} \
                                -v "${MLFLOW_DB_DIR}/database":/mlflow_data/database \
                                -v "${MLFLOW_DB_DIR}/artifacts":/mlflow_data/artifacts \
                                --volumes-from data_container \
                                ${BASE_IMAGE}:${TAG} \
                                tail -f /dev/null
                            
                            # Setup permissions
                            docker exec model_trainer bash -c "
                                mkdir -p /app/mlruns && \
                                chmod -R 777 /app /mlflow_data
                            "
                        """
                        
                        // Initialize MLflow experiment and run training
                        sh '''
                            docker exec \
                                -e MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
                                -e MLFLOW_EXPERIMENT_NAME=${EXPERIMENT_NAME} \
                                model_trainer \
                                python -c "
import mlflow
import os
import sys

# Setup MLflow
mlflow.set_experiment(os.getenv('MLFLOW_EXPERIMENT_NAME', 'churn_prediction'))

# Run main script within MLflow
with mlflow.start_run():
    sys.path.append('/app')
    import main
    main.run_full_pipeline('churn-bigml-80.csv', 'churn-bigml-20.csv')
" 2>&1 | tee ${WORKSPACE}/training.log
                        '''
                        
                        // Save and push model
                        sh """
                            docker commit model_trainer ${MODEL_IMAGE}:${TAG}-mlflow
                            docker push ${MODEL_IMAGE}:${TAG}-mlflow
                        """
                    } catch (Exception e) {
                        echo "Training failed: ${e.message}"
                        sh 'docker logs model_trainer || true'
                        throw e
                    }
                }
            }
        }
        
        stage('Evaluate Model') {
            steps {
                timeout(time: 2, unit: 'MINUTES') {
                    script {
                        sh '''
                            echo "=== Model Evaluation Results ==="
                            docker exec \
                                -e MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
                                model_trainer \
                                python -c "
import mlflow
from mlflow.tracking import MlflowClient

try:
    client = MlflowClient()
    experiment = client.get_experiment_by_name('${EXPERIMENT_NAME}')
    
    if experiment:
        runs = client.search_runs(experiment_ids=[experiment.experiment_id])
        if runs:
            run = runs[0]
            print(f'\\nRun ID: {run.info.run_id}')
            print(f'Status: {run.info.status}')
            print('\\nMetrics:')
            for key, value in run.data.metrics.items():
                print(f'{key}: {value:.4f}')
        else:
            print('No runs found')
    else:
        print('Experiment not found')
except Exception as e:
    print(f'Error: {str(e)}')
"
                        '''
                    }
                }
            }
        }
    }
    
    post {
        always {
            sh """
                docker rm -f model_trainer data_container || true
                docker logout || true
                docker system prune -f || true
                
                if [ -d "${MLFLOW_DB_DIR}" ]; then
                    cd "${WORKSPACE_SAFE}" && tar -czf mlflow_artifacts.tar.gz -C "${MLFLOW_DB_DIR}" . || true
                fi
            """
            
            archiveArtifacts artifacts: 'mlflow_artifacts.tar.gz,training.log', allowEmptyArchive: true
        }
        success {
            echo "Pipeline completed successfully. MLflow UI available at http://localhost:5001"
        }
        failure {
            sh 'docker logs mlflow_server || true'
            sh 'docker logs model_trainer || true'
            echo "Pipeline failed. Check logs for details."
        }
    }
}
