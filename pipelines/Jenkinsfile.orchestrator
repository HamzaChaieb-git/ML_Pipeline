pipeline {
    agent any
    
    environment {
        BASE_IMAGE = 'hamzachaieb01/ml-pipeline'
        DATA_IMAGE = 'hamzachaieb01/ml-data'
        MODEL_IMAGE = 'hamzachaieb01/ml-model'
        TAG = 'latest'
        WORKSPACE_SAFE = sh(script: 'echo "${WORKSPACE}" | sed "s/ /\\ /g"', returnStdout: true).trim()
        MLFLOW_TRACKING_URI = 'http://localhost:5001'  // Updated to match your local MLflow server
        EXPERIMENT_NAME = 'churn_prediction'
        MLFLOW_DB_PATH = "${WORKSPACE}/mlflow.db"  // SQLite database path
    }
    
    options {
        timeout(time: 2, unit: 'HOURS')
        disableConcurrentBuilds()
    }
    
    stages {
        stage('Setup MLflow') {
            steps {
                script {
                    // Create network for MLflow communication if it doesn't exist
                    sh '''
                        docker network create mlflow_net || true
                        
                        # Create directory for MLflow database
                        mkdir -p ${WORKSPACE}/mlflow_data
                        
                        # Start MLflow server if not running
                        docker ps -a | grep mlflow_server && docker rm -f mlflow_server || true
                        docker run -d --name mlflow_server \
                            --network mlflow_net \
                            -p 5001:5001 \
                            -v ${WORKSPACE}/mlflow_data:/mlflow_data \
                            ${BASE_IMAGE}:${TAG} \
                            mlflow ui \
                            --backend-store-uri sqlite:///mlflow_data/mlflow.db \
                            --host 0.0.0.0 \
                            --port 5001
                        
                        # Wait for MLflow server to start
                        sleep 10
                    '''
                }
            }
        }

        stage('Docker Login') {
            steps {
                withCredentials([usernamePassword(credentialsId: 'docker-hub-credentials', 
                                usernameVariable: 'DOCKER_USERNAME', 
                                passwordVariable: 'DOCKER_PASSWORD')]) {
                    sh 'echo $DOCKER_PASSWORD | docker login -u $DOCKER_USERNAME --password-stdin'
                }
            }
        }
        
        stage('Pull Images') {
            steps {
                script {
                    sh """
                        docker pull ${BASE_IMAGE}:${TAG}
                        docker pull ${DATA_IMAGE}:${TAG}
                    """
                }
            }
        }
        
        stage('Train and Track Model') {
            steps {
                script {
                    try {
                        sh """
                            # Create data container
                            docker create --name data_container \
                                --network mlflow_net \
                                ${DATA_IMAGE}:${TAG}
                            
                            # Run model trainer container
                            docker run -d --name model_trainer \
                                --network mlflow_net \
                                -e MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
                                -e EXPERIMENT_NAME=${EXPERIMENT_NAME} \
                                -v ${WORKSPACE}/mlflow_data:/mlflow_data \
                                --volumes-from data_container \
                                ${BASE_IMAGE}:${TAG} \
                                tail -f /dev/null
                            
                            # Create directories and set permissions
                            docker exec model_trainer mkdir -p /app/mlruns
                            docker exec model_trainer chmod -R 777 /app
                            
                            # Run the full pipeline with MLflow tracking
                            docker exec -e MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
                                -e MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow_data/mlflow.db \
                                model_trainer python -u -m main all > /app/pipeline_output.txt 2>&1
                            
                            # Save pipeline output
                            docker cp model_trainer:/app/pipeline_output.txt ${WORKSPACE}/pipeline_output.txt
                            
                            # Save model artifacts
                            docker exec model_trainer bash -c "cp -r /app/mlruns/* /mlflow_data/artifacts/ || true"
                            
                            # Commit and push updated image with MLflow artifacts
                            docker commit model_trainer ${MODEL_IMAGE}:${TAG}-mlflow
                            docker push ${MODEL_IMAGE}:${TAG}-mlflow
                        """
                        
                        // Display pipeline output
                        sh 'cat ${WORKSPACE}/pipeline_output.txt'
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error "Pipeline failed: ${e.message}"
                    }
                }
            }
        }
        
        stage('Evaluate Results') {
            steps {
                script {
                    // Extract and display metrics from MLflow
                    sh '''
                        echo "=== Model Evaluation Results ==="
                        docker exec model_trainer python -c "
import mlflow
import json

client = mlflow.tracking.MlflowClient()
experiment = client.get_experiment_by_name('${EXPERIMENT_NAME}')
if experiment:
    runs = client.search_runs(experiment_ids=[experiment.experiment_id])
    if runs:
        latest_run = runs[0]
        metrics = latest_run.data.metrics
        print(f'\\nLatest Run Metrics:')
        print(f'Accuracy: {metrics.get(\"accuracy\", \"N/A\"):.4f}')
        print(f'ROC AUC: {metrics.get(\"roc_auc\", \"N/A\"):.4f}')
        print(f'F1 Score: {metrics.get(\"f1\", \"N/A\"):.4f}')
        print(f'Precision: {metrics.get(\"precision\", \"N/A\"):.4f}')
        print(f'Recall: {metrics.get(\"recall\", \"N/A\"):.4f}')
        
        # Get run ID and status
        print(f'\\nRun Details:')
        print(f'Run ID: {latest_run.info.run_id}')
        print(f'Status: {latest_run.info.status}')
"
                    '''
                }
            }
        }
    }
    
    post {
        always {
            sh '''
                # Keep MLflow server running but clean up other containers
                docker rm -f model_trainer || true
                docker rm -f data_container || true
                docker logout
                docker system prune -f
                
                # Archive pipeline output
                tar -czf mlflow_artifacts.tar.gz -C ${WORKSPACE}/mlflow_data .
            '''
            archiveArtifacts artifacts: 'mlflow_artifacts.tar.gz,pipeline_output.txt', allowEmptyArchive: true
            cleanWs(patterns: [[pattern: 'mlflow_data/**', type: 'EXCLUDE']])
        }
        success {
            echo "Pipeline completed successfully. MLflow UI available at ${MLFLOW_TRACKING_URI}"
        }
        failure {
            echo "Pipeline failed. Check logs for details."
        }
    }
}
