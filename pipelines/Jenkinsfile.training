// pipelines/Jenkinsfile.training
pipeline {
    agent any
    
    environment {
        BASE_IMAGE = 'hamzachaieb01/ml-pipeline'
        DATA_IMAGE = 'hamzachaieb01/ml-data'
        MODEL_IMAGE = 'hamzachaieb01/ml-model'
        TAG = 'latest'
        MLFLOW_TRACKING_URI = 'http://mlflow:5000'
        WORKSPACE_SAFE = sh(script: 'echo "${WORKSPACE}" | sed "s/ /\\ /g"', returnStdout: true).trim()
    }
    
    options {
        timeout(time: 1, unit: 'HOURS')
        disableConcurrentBuilds()
    }
    
    stages {
        stage('Docker Login') {
            steps {
                withCredentials([usernamePassword(credentialsId: 'docker-hub-credentials', 
                               usernameVariable: 'DOCKER_USERNAME', 
                               passwordVariable: 'DOCKER_PASSWORD')]) {
                    sh 'echo $DOCKER_PASSWORD | docker login -u $DOCKER_USERNAME --password-stdin'
                }
            }
        }
        
        stage('Pull Images') {
            steps {
                script {
                    sh """
                        docker pull ${BASE_IMAGE}:${TAG}
                        docker pull ${DATA_IMAGE}:${TAG}
                    """
                }
            }
        }
        
        stage('Train Model') {
            steps {
                script {
                    try {
                        // Create data container
                        sh """
                            docker create --name data_container ${DATA_IMAGE}:${TAG}
                        """
                        
                        // Create and start model trainer container
                        sh """
                            # Create container with a sleep command to keep it running
                            docker run -d --name model_trainer \
                                -e MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
                                --volumes-from data_container \
                                ${BASE_IMAGE}:${TAG} \
                                tail -f /dev/null
                            
                            # Train model
                            docker exec model_trainer python -m main train_model
                            
                            # Evaluate model and capture all metrics
                            docker exec model_trainer python -u -m main evaluate_model | tee model_output.txt
                            
                            # Save model
                            docker exec model_trainer python -m main save_model
                        """
                        
                        // Get training timestamp
                        def timestamp = sh(script: 'date +%Y%m%d_%H%M%S', returnStdout: true).trim()
                        
                        // Extract metrics using Python for better parsing
                        def metrics = sh(
                            script: '''
                                docker exec model_trainer python3 -c "
import re
import json

def parse_metrics(file_path):
    with open(file_path, 'r') as f:
        content = f.read()
    
    # Extract all metrics
    metrics = {}
    
    # Get overall accuracy
    accuracy_match = re.search(r'accuracy\\s*:\\s*(\\d+\\.\\d+)', content)
    if accuracy_match:
        metrics['accuracy'] = float(accuracy_match.group(1))
    
    # Get class specific metrics
    for class_label in ['0', '1']:
        class_metrics = re.findall(
            f'{class_label}\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)', 
            content
        )
        if class_metrics:
            precision, recall, f1 = map(float, class_metrics[0])
            metrics[f'precision_{class_label}'] = precision
            metrics[f'recall_{class_label}'] = recall
            metrics[f'f1_{class_label}'] = f1
    
    # Get confusion matrix
    matrix_lines = re.findall(r'Confusion Matrix:\\n\\s*(\\[.*?\\])', content, re.DOTALL)
    if matrix_lines:
        matrix_str = matrix_lines[0].replace('\\n', '').replace(' ', '')
        metrics['confusion_matrix'] = matrix_str
    
    return metrics

metrics = parse_metrics('model_output.txt')
print(json.dumps(metrics))
"
                            ''',
                            returnStdout: true
                        ).trim()
                        
                        // Parse the JSON metrics
                        def metricsMap = readJSON text: metrics
                        
                        // Create label arguments for all metrics
                        def labelArgs = metricsMap.collect { k, v ->
                            "-c 'LABEL model.${k}=${v}'"
                        }.join(' ')
                        
                        // Save metrics for artifacts
                        sh """
                            mkdir -p metrics
                            cat << EOF > metrics/model_metrics.json
${metrics}
EOF
                        """
                        
                        // Create and push model images with metrics
                        withCredentials([usernamePassword(credentialsId: 'docker-hub-credentials', 
                                       usernameVariable: 'DOCKER_USERNAME', 
                                       passwordVariable: 'DOCKER_PASSWORD')]) {
                            sh """
                                # Re-login to ensure credentials are fresh
                                echo "\$DOCKER_PASSWORD" | docker login -u "\$DOCKER_USERNAME" --password-stdin
                                
                                # Commit container with all metrics
                                docker commit \
                                    -c 'LABEL training.timestamp=${timestamp}' \
                                    ${labelArgs} \
                                    model_trainer ${MODEL_IMAGE}:${TAG}
                                
                                # Create tags for key metrics
                                docker tag ${MODEL_IMAGE}:${TAG} ${MODEL_IMAGE}:accuracy-${metricsMap.accuracy}
                                docker tag ${MODEL_IMAGE}:${TAG} ${MODEL_IMAGE}:f1_weighted-${metricsMap.f1_1}
                                docker tag ${MODEL_IMAGE}:${TAG} ${MODEL_IMAGE}:${timestamp}
                                
                                # Push all tags
                                docker push ${MODEL_IMAGE}:${TAG}
                                docker push ${MODEL_IMAGE}:accuracy-${metricsMap.accuracy}
                                docker push ${MODEL_IMAGE}:f1_weighted-${metricsMap.f1_1}
                                docker push ${MODEL_IMAGE}:${timestamp}
                                
                                echo "âœ… Training complete. Images pushed:"
                                echo "- ${MODEL_IMAGE}:${TAG}"
                                echo "- ${MODEL_IMAGE}:accuracy-${metricsMap.accuracy}"
                                echo "- ${MODEL_IMAGE}:f1_weighted-${metricsMap.f1_1}"
                                echo "- ${MODEL_IMAGE}:${timestamp}"
                                echo "\\nModel Metrics:"
                                echo '${metrics}' | python3 -m json.tool
                            """
                        }
                        
                        archiveArtifacts artifacts: 'metrics/*.json', fingerprint: true
                        
                    } catch (Exception e) {
                        currentBuild.result = 'FAILURE'
                        error "Model training failed: ${e.message}"
                    }
                }
            }
        }
    }
    
    post {
        always {
            sh '''
                docker rm -f model_trainer || true
                docker rm -f data_container || true
                docker logout
                docker system prune -f
                rm -f model_output.txt
            '''
            cleanWs()
        }
        success {
            echo "Training pipeline completed successfully"
        }
        failure {
            echo "Training pipeline failed"
        }
    }
}
